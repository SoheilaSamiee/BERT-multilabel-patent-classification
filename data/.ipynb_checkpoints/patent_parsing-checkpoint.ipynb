{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ...\n",
      "Filename: tmp/ipg210105_3.xml\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "No Patent\n",
      "Success Count: 3\n",
      "Error Count: 0\n",
      "Saved to file!\n"
     ]
    }
   ],
   "source": [
    "# Inspired by https://github.com/lettergram/parse-uspto-xml\n",
    "\n",
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "import html\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Function \n",
    "# ---------------------------------------------------------------------------\n",
    "def parse_patent(bs):\n",
    "    \"\"\"\n",
    "    Parses a US patent in a BeautifulSoup object.\n",
    "    Only keeping the elements useful for our next text classification task\n",
    "    \"\"\"\n",
    "    \n",
    "    publication_title = bs.find('invention-title').text\n",
    "    application_type = bs.find('application-reference')['appl-type']\n",
    "\n",
    "    # International Patent Classification (IPC) Docs:\n",
    "    sections = {}\n",
    "    for classes in bs.find_all('classifications-ipcr'):\n",
    "        for el in classes.find_all('classification-ipcr'):\n",
    "            section = el.find('section').text                            \n",
    "            sections[section] = True\n",
    "    #print(list(sections.keys()))\n",
    "    section_list = list(sections.keys())\n",
    "    section_string =\",\".join(section_list)\n",
    "        \n",
    "    regex = re.compile('[^a-zA-Z]')  # remove all but alphabetic (even numbers -- change to '[^a-zA-Z0-9]' to keep numbers)\n",
    "    \n",
    "        \n",
    "    abstracts = ''\n",
    "    for el in bs.find_all('abstract'):\n",
    "        abstracts += el.text.strip('\\n')\n",
    "        abstracts = regex.sub(' ', abstracts)\n",
    "        abstracts = re.sub(r'\\b\\w{1,2}\\b', '', abstracts)  # remove words of length < 3\n",
    "\n",
    "    \n",
    "    descriptions = ''\n",
    "    for el in bs.find_all('description'):\n",
    "        descriptions += el.text.strip('\\n')\n",
    "        descriptions = regex.sub(' ', descriptions)\n",
    "        descriptions = re.sub(r'\\b\\w{1,2}\\b', '', descriptions)  # remove words of length < 3\n",
    "\n",
    "    \n",
    "    claims = ''\n",
    "    for el in bs.find_all('claim'):\n",
    "        claims += el.text.strip('\\n')\n",
    "        claims = regex.sub(' ', claims)\n",
    "        claims = re.sub(r'\\b\\w{1,2}\\b', '', claims)  # remove words of length < 3\n",
    "        #print(claims)\n",
    "    \n",
    "    # output in the dictionary format\n",
    "    uspto_patent = {\n",
    "        \"sections\": [section_string], \n",
    "        \"publication_title\": regex.sub(' ', publication_title.replace(\"\\n\",\" \")),\n",
    "        \"application_type\": regex.sub(' ', application_type.replace(\"\\n\",\" \")),\n",
    "        \"abstract\": abstracts.replace(\"\\n\",\" \"), \n",
    "        \"descriptions\": descriptions.replace(\"\\n\",\" \"), \n",
    "        \"claims\": claims.replace(\"\\n\",\" \")\n",
    "    }        \n",
    "\n",
    "    return uspto_patent\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "filenames = 'tmp/ipg210105_3.xml'\n",
    "\n",
    "print('Start ...')    \n",
    "all_patents = pd.DataFrame()\n",
    "n_file = 1\n",
    "count = 1\n",
    "success_count = 0\n",
    "errors = []\n",
    "\n",
    "print(\"Filename:\", filenames)\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "\n",
    "xml_text = html.unescape(open(filenames, 'r').read())\n",
    "\n",
    "for patent in xml_text.split(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\"):\n",
    "    \n",
    "    if patent is None or patent == \"\":\n",
    "        print('No Patent')\n",
    "        continue\n",
    "\n",
    "    bs = BeautifulSoup(patent)\n",
    "\n",
    "    # Skip documents which do not have classification-ipcr\n",
    "    if bs.find('classification-ipcr') is None:\n",
    "        #print('No classification!')\n",
    "        continue \n",
    "\n",
    "    application = bs.find('us-patent-application')\n",
    "    if application is None: # If no application, search for grant\n",
    "        application = bs.find('us-patent-grant')\n",
    "\n",
    "        \n",
    "    # Setting the title\n",
    "    title = \"None\"\n",
    "    try:\n",
    "        title = application.find('invention-title').text\n",
    "    except Exception as e:          \n",
    "        print(\"Error\", count, e)\n",
    "\n",
    "    # parsing the patent\n",
    "    try: \n",
    "        uspto_patent = parse_patent(application)       \n",
    "        new_patent = pd.DataFrame.from_dict(data=uspto_patent)\n",
    "        \n",
    "        if all_patents.empty:\n",
    "            all_patents = new_patent            \n",
    "        else:\n",
    "            frames = [all_patents, new_patent]\n",
    "            all_patents = pd.concat(frames)                \n",
    "        success_count += 1\n",
    "    except Exception as e:\n",
    "        exception_tuple = (count, title, e)\n",
    "        errors.append(exception_tuple)\n",
    "\n",
    "    if (success_count+len(errors)) % 50 == 0:\n",
    "        print(count, filenames, title)\n",
    "        #new_patent.describe()\n",
    "    count += 1\n",
    "\n",
    "#print(\"\\n\\nErrors\\n------------------------\\n\")\n",
    "#for e in errors:\n",
    "#    print(e)\n",
    "    \n",
    "print(\"Success Count:\", success_count)\n",
    "print(\"Error Count:\", len(errors))\n",
    "\n",
    "#all_patents.to_csv('all_patents_ipg210105.tsv', index=False, sep=\"\\t\")\n",
    "all_patents.to_csv('all_patents_ipg210105.csv', index=False)\n",
    "#all_patents.to_csv('tmp/all_patents_ipg210105_3.csv', index=False)\n",
    "\n",
    "print('Saved to file!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A')\n",
    "vec = list('A','B')\n",
    "print(vec)\n",
    "\n",
    "print(str(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaEnv_Soheila",
   "language": "python",
   "name": "condaenv_soheila"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
